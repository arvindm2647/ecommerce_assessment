# E-commerce Data System - Fullstack Assessment

A comprehensive data system for e-commerce analytics featuring ETL pipeline, SQL queries, REST APIs, file processing, and Apache Airflow orchestration.

##  Table of Contents
- [Project Overview](#project-overview)
- [Project Structure](#project-structure)
- [Technologies Used](#technologies-used)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Running the Application](#running-the-application)
- [API Documentation](#api-documentation)
- [SQL Queries](#sql-queries)
- [Airflow Orchestration](#airflow-orchestration)
- [Testing](#testing)
- [Project Features](#project-features)

---

## Project Overview

This project implements a complete e-commerce data system with the following components:

1. **ETL Pipeline** - Automated data loading with validation
2. **SQL Analytics** - Business intelligence queries
3. **REST API** - Flask-based API with 5 endpoints
4. **File Processor** - Async file processing system (Node.js to Python conversion)
5. **Airflow DAG** - Workflow orchestration (Bonus Task)

---

##  Project Structure

```
ecommerce-data-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py           # Flask app initialization
â”‚   â”œâ”€â”€ routes.py             # API endpoints
â”‚   â”œâ”€â”€ database.py           # Database connection management
â”‚   â””â”€â”€ validators.py         # Input validation functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Source CSV files
â”‚   â”‚   â”œâ”€â”€ users.csv
â”‚   â”‚   â”œâ”€â”€ products.csv
â”‚   â”‚   â”œâ”€â”€ orders.csv
â”‚   â”‚   â”œâ”€â”€ order_items.csv
â”‚   â”‚   â””â”€â”€ events.csv
â”‚   â””â”€â”€ invalid/              # Invalid rows logged here
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ecommerce_etl_dag.py  # Airflow DAG definition
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql            # Database schema
â”‚   â””â”€â”€ queries.sql           # Analytics queries
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl.py                # ETL pipeline script
â”œâ”€â”€ input_files/              # File processor input
â”œâ”€â”€ processed_files/          # Successfully processed files
â”œâ”€â”€ error_files/              # Failed file processing
â”œâ”€â”€ output/
â”‚   â””â”€â”€ daily_revenue.csv     # Generated by Airflow
â”œâ”€â”€ app.py                    # Flask application entry point
â”œâ”€â”€ process_one_file.py       # File processor (Node.js conversion)
â”œâ”€â”€ test_api.py               # API testing script
â”œâ”€â”€ docker-compose.yml        # Airflow Docker configuration
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

---

## ğŸ› ï¸ Technologies Used

- **Backend:** Python 3.10
- **Web Framework:** Flask 3.0
- **Database:** PostgreSQL 18
- **Workflow Orchestration:** Apache Airflow 2.7.3
- **Data Processing:** pandas, psycopg2
- **Containerization:** Docker, Docker Compose
- **Libraries:** python-dotenv, flask-cors, asyncio

---

##  Prerequisites

- Python 3.8 or higher
- PostgreSQL 18
- Docker Desktop (for Airflow)
- pip (Python package manager)
- Git

---

##  Setup Instructions

### 1. Clone the Repository

```bash
git clone <repository-url>
cd ecommerce-data-system
```

### 2. Create Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Database Setup

**Create PostgreSQL Database:**

```bash
# Using psql
psql -U postgres
CREATE DATABASE ecommerce_db;
\q

# Or use pgAdmin GUI
```

**Execute Schema:**

Option 1 - Command Line:
```bash
psql -U postgres -d ecommerce_db -f sql/schema.sql
```

Option 2 - pgAdmin:
1. Open pgAdmin
2. Connect to PostgreSQL server
3. Right-click on `ecommerce_db` â†’ Query Tool
4. Open and execute `sql/schema.sql`

### 5. Environment Configuration

Create a `.env` file in project root:

```env
DB_HOST=localhost
DB_PORT=5434
DB_NAME=ecommerce_db
DB_USER=postgres
DB_PASSWORD=7447
```

**Note:** Update `DB_PORT` and `7447` with your actual PostgreSQL configuration.

### 6. Verify PostgreSQL Connection

Check your PostgreSQL port:
1. Open pgAdmin
2. Right-click PostgreSQL server â†’ Properties â†’ Connection
3. Note the port number (usually 5432 or 5434)

---

##  Running the Application

### Step 1: Load Data (ETL Pipeline)

```bash
python src/etl.py
```

**Expected Output:**
```
Starting ETL pipeline...
Database connection established
Processing users...
Loaded 10 rows from users.csv
Successfully inserted 10 rows into users
Processing products...
Loaded 15 rows from products.csv
Successfully inserted 15 rows into products
...
ETL pipeline completed successfully!
```

**What it does:**
- Loads CSV files into PostgreSQL
- Validates data (duplicates, formats, constraints)
- Logs invalid rows to `data/invalid/`

### Step 2: Start Flask API

```bash
python app.py
```

**Server runs at:** `http://127.0.0.1:5000`

### Step 3: Run File Processor

```bash
python process_one_file.py
```

Place files in `input_files/` folder. Processed files move to `processed_files/`.

### Step 4: Start Airflow (Bonus)

**Start Docker Desktop first!**

```bash
docker-compose up -d
```

**Access Airflow UI:**
- URL: `http://localhost:8080`
- Username: `admin`
- Password: `admin123`

**Trigger DAG:**
1. Enable `ecommerce_etl_pipeline` (toggle switch)
2. Click play button to trigger
3. Monitor task execution

---

##  API Documentation

### Base URL
```
http://127.0.0.1:5000
```

### Endpoints

#### 1. Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-17T10:30:00"
}
```

---

#### 2. Get User Orders
```http
GET /orders?user_id=1
```

**Parameters:**
- `user_id` (required): Integer

**Response:**
```json
{
  "orders": [
    {
      "order_id": 1,
      "order_date": "2024-01-20T10:30:00",
      "status": "paid",
      "total_amount": 1329.98,
      "items": [
        {
          "product_id": 1,
          "product_name": "Laptop Pro 15",
          "quantity": 1,
          "unit_price": 1299.99,
          "subtotal": 1299.99
        }
      ]
    }
  ]
}
```

---

#### 3. Daily Revenue
```http
GET /metrics/daily-revenue?from=2024-01-01&to=2024-12-31
```

**Parameters:**
- `from` (required): Date in YYYY-MM-DD format
- `to` (required): Date in YYYY-MM-DD format

**Response:**
```json
{
  "daily_revenue": [
    {
      "order_day": "2024-01-20",
      "daily_revenue": 1329.98,
      "total_orders": 1
    }
  ]
}
```

---

#### 4. Top Products
```http
GET /products/top?days=365&limit=5
```

**Parameters:**
- `days` (optional): Number of days to analyze (default: 7)
- `limit` (optional): Number of products to return (default: 5)

**Response:**
```json
{
  "top_products": [
    {
      "product_id": 1,
      "product_name": "Laptop Pro 15",
      "category": "Electronics",
      "total_units_sold": 3,
      "total_revenue": 3899.97
    }
  ]
}
```

---

#### 5. Ingest Events
```http
POST /ingest/events
Content-Type: application/json
```

**Request Body:**
```json
[
  {
    "user_id": 1,
    "event_type": "page_view",
    "product_id": 5,
    "event_timestamp": "2024-10-15T10:30:00"
  },
  {
    "user_id": 2,
    "event_type": "add_to_cart",
    "product_id": 3,
    "event_timestamp": "2024-10-15T10:35:00"
  }
]
```

**Valid event_type values:**
- `page_view`
- `add_to_cart`
- `purchase`

**Response:**
```json
{
  "inserted": 2,
  "rejected": 0,
  "invalid_events": []
}
```

---

##  SQL Queries

Located in `sql/queries.sql`

### Query 1: Daily Revenue
```sql
SELECT 
    DATE(order_date) AS order_day,
    SUM(total_amount) AS daily_revenue,
    COUNT(order_id) AS total_orders
FROM orders
WHERE status = 'paid'
GROUP BY DATE(order_date)
ORDER BY order_day DESC;
```

**Purpose:** Calculate total revenue per day for paid orders.

---

### Query 2: Top 5 Products by Revenue
```sql
SELECT 
    p.product_id,
    p.product_name,
    SUM(oi.quantity) AS total_units_sold,
    SUM(oi.quantity * oi.unit_price) AS total_revenue
FROM products p
JOIN order_items oi ON p.product_id = oi.product_id
JOIN orders o ON oi.order_id = o.order_id
WHERE o.status = 'paid'
GROUP BY p.product_id, p.product_name
ORDER BY total_revenue DESC
LIMIT 5;
```

**Purpose:** Identify best-selling products by revenue.

---

### Query 3: User Retention (D+7)
```sql
WITH user_signups AS (
    SELECT 
        DATE(signup_date) AS signup_day,
        user_id
    FROM users
),
day7_activity AS (
    SELECT 
        us.signup_day,
        us.user_id,
        CASE 
            WHEN EXISTS (
                SELECT 1 
                FROM events e 
                WHERE e.user_id = us.user_id 
                AND DATE(e.event_timestamp) = us.signup_day + INTERVAL '7 days'
            ) THEN 1 
            ELSE 0 
        END AS returned_d7
    FROM user_signups us
)
SELECT 
    signup_day,
    COUNT(user_id) AS total_signups,
    SUM(returned_d7) AS returned_users,
    ROUND(100.0 * SUM(returned_d7) / COUNT(user_id), 2) AS retention_percentage
FROM day7_activity
GROUP BY signup_day
ORDER BY signup_day;
```

**Purpose:** Calculate percentage of users who return 7 days after signup.

---

### Query 4: Order Funnel
```sql
WITH funnel_counts AS (
    SELECT 
        COUNT(CASE WHEN event_type = 'page_view' THEN 1 END) AS page_views,
        COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END) AS add_to_carts,
        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases
    FROM events
)
SELECT 
    'page_view' AS stage,
    page_views AS count,
    100.0 AS conversion_rate_from_previous,
    ROUND(100.0 * page_views / page_views, 2) AS conversion_rate_from_start
FROM funnel_counts

UNION ALL

SELECT 
    'add_to_cart' AS stage,
    add_to_carts AS count,
    ROUND(100.0 * add_to_carts / NULLIF(page_views, 0), 2) AS conversion_rate_from_previous,
    ROUND(100.0 * add_to_carts / NULLIF(page_views, 0), 2) AS conversion_rate_from_start
FROM funnel_counts

UNION ALL

SELECT 
    'purchase' AS stage,
    purchases AS count,
    ROUND(100.0 * purchases / NULLIF(add_to_carts, 0), 2) AS conversion_rate_from_previous,
    ROUND(100.0 * purchases / NULLIF(page_views, 0), 2) AS conversion_rate_from_start
FROM funnel_counts;
```

**Purpose:** Analyze conversion rates through purchase funnel.

---

## Airflow Orchestration (Bonus)

### DAG: `ecommerce_etl`

**Schedule:** Daily (`@daily`)

**Tasks:**
1. **load_csv_to_database** - Loads all CSV files into PostgreSQL
2. **build_sql_views** - Creates analytical views
3. **run_data_quality_tests** - Validates data integrity
4. **materialize_daily_revenue** - Exports revenue to CSV

**Task Flow:**
```
load_csv_to_database 
build_sql_views
run_data_quality_tests
materialize_daily_revenue
```

### Data Quality Tests

1. Check users table is populated
2. Check products table is populated
3. Validate no negative order amounts
4. Validate event types are valid
5. Check for orphan order items

**Failure Behavior:** Pipeline stops at failed task

### Output

Generated file: `output/daily_revenue.csv`

Contains daily revenue aggregation for reporting.

---

##  Testing

### Test API Endpoints

**Manual Testing:**

```bash
# Health check
curl http://127.0.0.1:5000/health

# Get orders
curl http://127.0.0.1:5000/orders?user_id=1

# Daily revenue
curl "http://127.0.0.1:5000/metrics/daily-revenue?from=2024-01-01&to=2024-12-31"

# Top products
curl http://127.0.0.1:5000/products/top?days=365&limit=5
```

**Automated Testing:**

```bash
pip install requests
python test_api.py
```

### Test SQL Queries

Execute queries in pgAdmin:
1. Open pgAdmin
2. Right-click `ecommerce_db` â†’ Query Tool
3. Copy queries from `sql/queries.sql`
4. Execute and verify results

### Test File Processor

```bash
# Create test file
echo "Test data" > input_files/test.txt

# Run processor
python process_one_file.py

# Check processed_files folder
```

---

## Project Features

### ETL Pipeline Features
-  Modular validation functions
-  Duplicate detection (primary keys, emails)
-  ISO8601 timestamp validation
-  Non-negative number validation
-  Invalid row logging to CSV
-  Foreign key constraint handling
-  Batch processing with psycopg2

### API Features
-  RESTful design
-  Input validation
-  Database connection pooling
-  Error handling with HTTP status codes
-  JSON request/response format
-  CORS enabled

### File Processor Features
-  Async file processing
-  Job creation and tracking
-  File movement (input â†’ processed/error)
-  Process metrics (TPS, elapsed time)
-  Error handling and logging
-  Node.js to Python conversion

### Airflow Features
-  Automated workflow orchestration
-  Task dependencies
-  Data quality testing
-  Error handling and retries
-  Monitoring and logging
-  CSV export for reporting

---

##  Data Validation Rules

- No duplicate primary keys
- Timestamps must be ISO8601 format
- Prices and quantities must be non-negative (â‰¥ 0)
- Email must contain @ and .
- Event types: `page_view`, `add_to_cart`, `purchase`
- Order status: `pending`, `paid`, `cancelled`, `shipped`

---

##  Troubleshooting

### Database Connection Issues

**Problem:** Can't connect to PostgreSQL

**Solutions:**
1. Verify PostgreSQL is running
2. Check port number in `.env` (5432 or 5434)
3. Confirm database credentials
4. Test connection in pgAdmin

### ETL Issues

**Problem:** CSV files not loading

**Solutions:**
1. Ensure CSV files are in `data/raw/` folder
2. Check invalid rows in `data/invalid/` folder
3. Verify database schema is created
4. Check file permissions

### API Issues

**Problem:** API not responding

**Solutions:**
1. Confirm Flask server is running on port 5000
2. Check database connection pool
3. Verify request parameters format
4. Check Flask terminal for error logs

### Airflow Issues

**Problem:** DAG not appearing

**Solutions:**
1. Ensure DAG file is in `dags/` folder
2. Check for Python syntax errors
3. Restart Docker containers: `docker-compose restart`
4. Check Airflow logs

**Problem:** Tasks failing

**Solutions:**
1. Check task logs in Airflow UI
2. Verify database connection settings in docker-compose.yml
3. Ensure `data/raw/` folder has CSV files
4. Check PostgreSQL is accessible from Docker

---

##  Assessment Completion

### Day 1: ETL & SQL 
- ETL Pipeline with validation
- SQL Analytics Queries (4 queries)

### Day 2: Flask APIs & Conversions 
- Flask API (5 endpoints)
- Node.js to Python conversion

### Day 3: Finalization 
- Complete documentation
- Testing scripts
- README with setup instructions

### Bonus: Airflow Orchestration 
- Complete DAG with 4 tasks
- Data quality testing
- CSV materialization

