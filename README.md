# E-commerce Data System - Fullstack Assessment

A comprehensive data system for e-commerce analytics featuring ETL pipeline, SQL queries, REST APIs, file processing, and Apache Airflow orchestration.

##  Table of Contents
- [Project Overview](#project-overview)
- [Project Structure](#project-structure)
- [Technologies Used](#technologies-used)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Running the Application](#running-the-application)
- [API Documentation](#api-documentation)
- [SQL Queries](#sql-queries)
- [Airflow Orchestration](#airflow-orchestration)
- [Testing](#testing)
- [Project Features](#project-features)

---

## Project Overview

This project implements a complete e-commerce data system with the following components:

1. **ETL Pipeline** - Automated data loading with validation
2. **SQL Analytics** - Business intelligence queries
3. **REST API** - Flask-based API with 5 endpoints
4. **File Processor** - Async file processing system (Node.js to Python conversion)
5. **Airflow DAG** - Workflow orchestration (Bonus Task)

---

##  Project Structure

```
ecommerce-data-system/
├── app/
│   ├── __init__.py           # Flask app initialization
│   ├── routes.py             # API endpoints
│   ├── database.py           # Database connection management
│   └── validators.py         # Input validation functions
├── data/
│   ├── raw/                  # Source CSV files
│   │   ├── users.csv
│   │   ├── products.csv
│   │   ├── orders.csv
│   │   ├── order_items.csv
│   │   └── events.csv
│   └── invalid/              # Invalid rows logged here
├── dags/
│   └── ecommerce_etl_dag.py  # Airflow DAG definition
├── sql/
│   ├── schema.sql            # Database schema
│   └── queries.sql           # Analytics queries
├── src/
│   └── etl.py                # ETL pipeline script
├── input_files/              # File processor input
├── processed_files/          # Successfully processed files
├── error_files/              # Failed file processing
├── output/
│   └── daily_revenue.csv     # Generated by Airflow
├── app.py                    # Flask application entry point
├── process_one_file.py       # File processor (Node.js conversion)
├── test_api.py               # API testing script
├── docker-compose.yml        # Airflow Docker configuration
├── .env                      # Environment variables
├── requirements.txt          # Python dependencies
└── README.md                 # This file
```

---

##  Technologies Used

- **Backend:** Python 3.10
- **Web Framework:** Flask 3.0
- **Database:** PostgreSQL 18
- **Workflow Orchestration:** Apache Airflow 2.7.3
- **Data Processing:** pandas, psycopg2
- **Containerization:** Docker, Docker Compose
- **Libraries:** python-dotenv, flask-cors, asyncio

---

##  Prerequisites

- Python 3.8 or higher
- PostgreSQL 18
- Docker Desktop (for Airflow)
- pip (Python package manager)
- Git

---

##  Setup Instructions

### 1. Clone the Repository

```bash
git clone <repository-url>
cd ecommerce-data-system
```

### 2. Create Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Database Setup

**Create PostgreSQL Database:**

```bash
# Using psql
psql -U postgres
CREATE DATABASE ecommerce_db;
\q

# Or use pgAdmin GUI
```

**Execute Schema:**

Option 1 - Command Line:
```bash
psql -U postgres -d ecommerce_db -f sql/schema.sql
```

Option 2 - pgAdmin:
1. Open pgAdmin
2. Connect to PostgreSQL server
3. Right-click on `ecommerce_db` → Query Tool
4. Open and execute `sql/schema.sql`

### 5. Environment Configuration

Create a `.env` file in project root:

```env
DB_HOST=localhost
DB_PORT=5434
DB_NAME=ecommerce_db
DB_USER=postgres
DB_PASSWORD=7447
```

**Note:** Update `DB_PORT` and `7447` with your actual PostgreSQL configuration.

### 6. Verify PostgreSQL Connection

Check your PostgreSQL port:
1. Open pgAdmin
2. Right-click PostgreSQL server → Properties → Connection
3. Note the port number (usually 5432 or 5434)

---

##  Running the Application

### Step 1: Load Data (ETL Pipeline)

```bash
python src/etl.py
```

**Expected Output:**
```
Starting ETL pipeline...
Database connection established
Processing users...
Loaded 10 rows from users.csv
Successfully inserted 10 rows into users
Processing products...
Loaded 15 rows from products.csv
Successfully inserted 15 rows into products
...
ETL pipeline completed successfully!
```

**What it does:**
- Loads CSV files into PostgreSQL
- Validates data (duplicates, formats, constraints)
- Logs invalid rows to `data/invalid/`

### Step 2: Start Flask API

```bash
python app.py
```

**Server runs at:** `http://127.0.0.1:5000`

### Step 3: Run File Processor

```bash
python process_one_file.py
```

Place files in `input_files/` folder. Processed files move to `processed_files/`.

### Step 4: Start Airflow (Bonus)

**Start Docker Desktop first!**

```bash
docker-compose up -d
```

**Access Airflow UI:**
- URL: `http://localhost:8080`
- Username: `admin`
- Password: `admin123`

**Trigger DAG:**
1. Enable `ecommerce_etl_pipeline` (toggle switch)
2. Click play button to trigger
3. Monitor task execution

---

##  API Documentation

### Base URL
```
http://127.0.0.1:5000
```

### Endpoints

#### 1. Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-17T10:30:00"
}
```

---

#### 2. Get User Orders
```http
GET /orders?user_id=1
```

**Parameters:**
- `user_id` (required): Integer

**Response:**
```json
{
  "orders": [
    {
      "order_id": 1,
      "order_date": "2024-01-20T10:30:00",
      "status": "paid",
      "total_amount": 1329.98,
      "items": [
        {
          "product_id": 1,
          "product_name": "Laptop Pro 15",
          "quantity": 1,
          "unit_price": 1299.99,
          "subtotal": 1299.99
        }
      ]
    }
  ]
}
```

---

#### 3. Daily Revenue
```http
GET /metrics/daily-revenue?from=2024-01-01&to=2024-12-31
```

**Parameters:**
- `from` (required): Date in YYYY-MM-DD format
- `to` (required): Date in YYYY-MM-DD format

**Response:**
```json
{
  "daily_revenue": [
    {
      "order_day": "2024-01-20",
      "daily_revenue": 1329.98,
      "total_orders": 1
    }
  ]
}
```

---

#### 4. Top Products
```http
GET /products/top?days=365&limit=5
```

**Parameters:**
- `days` (optional): Number of days to analyze (default: 7)
- `limit` (optional): Number of products to return (default: 5)

**Response:**
```json
{
  "top_products": [
    {
      "product_id": 1,
      "product_name": "Laptop Pro 15",
      "category": "Electronics",
      "total_units_sold": 3,
      "total_revenue": 3899.97
    }
  ]
}
```

---

#### 5. Ingest Events
```http
POST /ingest/events
Content-Type: application/json
```

**Request Body:**
```json
[
  {
    "user_id": 1,
    "event_type": "page_view",
    "product_id": 5,
    "event_timestamp": "2024-10-15T10:30:00"
  },
  {
    "user_id": 2,
    "event_type": "add_to_cart",
    "product_id": 3,
    "event_timestamp": "2024-10-15T10:35:00"
  }
]
```

**Valid event_type values:**
- `page_view`
- `add_to_cart`
- `purchase`

**Response:**
```json
{
  "inserted": 2,
  "rejected": 0,
  "invalid_events": []
}
```

---

##  SQL Queries

Located in `sql/queries.sql`

### Query 1: Daily Revenue
```sql
SELECT 
    DATE(order_date) AS order_day,
    SUM(total_amount) AS daily_revenue,
    COUNT(order_id) AS total_orders
FROM orders
WHERE status = 'paid'
GROUP BY DATE(order_date)
ORDER BY order_day DESC;
```

**Purpose:** Calculate total revenue per day for paid orders.

---

### Query 2: Top 5 Products by Revenue
```sql
SELECT 
    p.product_id,
    p.product_name,
    SUM(oi.quantity) AS total_units_sold,
    SUM(oi.quantity * oi.unit_price) AS total_revenue
FROM products p
JOIN order_items oi ON p.product_id = oi.product_id
JOIN orders o ON oi.order_id = o.order_id
WHERE o.status = 'paid'
    AND o.order_date >= (SELECT MAX(order_date) - INTERVAL '7 days' FROM orders)
GROUP BY p.product_id, p.product_name
ORDER BY total_revenue DESC
LIMIT 5;

```

**Purpose:** Identify best-selling products by revenue.

---

### Query 3: User Retention (D+7)
```sql
WITH user_signups AS (
    SELECT 
        DATE(signup_date) AS signup_day,
        user_id
    FROM users
),
day7_activity AS (
    SELECT 
        us.signup_day,
        us.user_id,
        CASE 
            WHEN EXISTS (
                SELECT 1 
                FROM events e 
                WHERE e.user_id = us.user_id 
                AND DATE(e.event_timestamp) = us.signup_day + INTERVAL '7 days'
            ) THEN 1 
            ELSE 0 
        END AS returned_d7
    FROM user_signups us
)
SELECT 
    signup_day,
    COUNT(user_id) AS total_signups,
    SUM(returned_d7) AS returned_users,
    ROUND(100.0 * SUM(returned_d7) / COUNT(user_id), 2) AS retention_percentage
FROM day7_activity
GROUP BY signup_day
ORDER BY signup_day;
```

**Purpose:** Calculate percentage of users who return 7 days after signup.

---

### Query 4: Order Funnel
```sql
WITH funnel_counts AS (
    SELECT 
        COUNT(CASE WHEN event_type = 'page_view' THEN 1 END) AS page_views,
        COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END) AS add_to_carts,
        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases
    FROM events
)
SELECT 
    'page_view' AS stage,
    page_views AS count,
    100.0 AS conversion_rate_from_previous,
    ROUND(100.0 * page_views / page_views, 2) AS conversion_rate_from_start
FROM funnel_counts

UNION ALL

SELECT 
    'add_to_cart' AS stage,
    add_to_carts AS count,
    ROUND(100.0 * add_to_carts / NULLIF(page_views, 0), 2) AS conversion_rate_from_previous,
    ROUND(100.0 * add_to_carts / NULLIF(page_views, 0), 2) AS conversion_rate_from_start
FROM funnel_counts

UNION ALL

SELECT 
    'purchase' AS stage,
    purchases AS count,
    ROUND(100.0 * purchases / NULLIF(add_to_carts, 0), 2) AS conversion_rate_from_previous,
    ROUND(100.0 * purchases / NULLIF(page_views, 0), 2) AS conversion_rate_from_start
FROM funnel_counts;
```

**Purpose:** Analyze conversion rates through purchase funnel.

---

## Airflow Orchestration (Bonus)

### DAG: `ecommerce_etl`

**Schedule:** Daily (`@daily`)

**Tasks:**
1. **load_csv_to_database** - Loads all CSV files into PostgreSQL
2. **build_sql_views** - Creates analytical views
3. **run_data_quality_tests** - Validates data integrity
4. **materialize_daily_revenue** - Exports revenue to CSV

**Task Flow:**
```
load_csv_to_database 
build_sql_views
run_data_quality_tests
materialize_daily_revenue
```

### Data Quality Tests

1. Check users table is populated
2. Check products table is populated
3. Validate no negative order amounts
4. Validate event types are valid
5. Check for orphan order items

**Failure Behavior:** Pipeline stops at failed task

### Output

Generated file: `output/daily_revenue.csv`

Contains daily revenue aggregation for reporting.

---

##  Testing

### Test API Endpoints

**Manual Testing:**

```bash
# Health check
curl http://127.0.0.1:5000/health

# Get orders
curl http://127.0.0.1:5000/orders?user_id=1

# Daily revenue
curl "http://127.0.0.1:5000/metrics/daily-revenue?from=2024-01-01&to=2024-12-31"

# Top products
curl http://127.0.0.1:5000/products/top?days=365&limit=5
```

**Automated Testing:**

```bash
pip install requests
python test_api.py
```

### Test SQL Queries

Execute queries in pgAdmin:
1. Open pgAdmin
2. Right-click `ecommerce_db` → Query Tool
3. Copy queries from `sql/queries.sql`
4. Execute and verify results

### Test File Processor

```bash
# Create test file
echo "Test data" > input_files/test.txt

# Run processor
python process_one_file.py

# Check processed_files folder
```

---


